{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "FOLDER_PATH = \"sentiment_datasets/projekt2_data\"\n",
    "\n",
    "data_df = load_data(FOLDER_PATH, df_delimiter=\",\")\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"Phrase\"].str.split().str.len().plot(kind=\"hist\", title=\"number of tokens in line distribution\", grid=True, figsize=(10,10), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_history(history, save_img=True):\n",
    "    nb = len(history)\n",
    "    nrows = nb // 2\n",
    "    ncols = nb % nrows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    for i, j in cartesian((np.arange(nrows), np.arange(ncols)))\n",
    "        ax[i][j].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_input_converter import get_proper_input_to_bert\n",
    "from layers import BertLayer, BertInputLayer\n",
    "from utils import get_proper_callback\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from livelossplot import PlotLossesKeras\n",
    "from utils_result import save_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = get_proper_input_to_bert(data_df, x_label=\"Phrase\", y_label=\"Sentiment\", max_len_seq=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_params = {\n",
    "    \"histogram_freq\": 1,\n",
    "    \"write_graph\": True,\n",
    "    \"write_images\": True\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    \"x\": x_train,\n",
    "    \"y\": y_train,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 50,\n",
    "    \"validation_data\": (x_val, y_val)    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import model_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bert_tokens_rnn = defaultdict(int)\n",
    "\n",
    "params_bert_tokens_rnn = {\n",
    "    \"is_bert\": True,\n",
    "    \"bert_input_layer\": BertInputLayer,\n",
    "    \"shape\": (1, ),\n",
    "    \"input_dtype\": x_train.dtype,\n",
    "    \"emb_layer\": BertLayer(trainable=True, dict_output=\"sequence_outputs\"),\n",
    "    \"out_units\": np.unique(y_train).shape[0],\n",
    "    \"out_activation\": \"softmax\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"optimizer\": \"rmsprop\"\n",
    "}\n",
    "\n",
    "model_bert_rnn = model_rnn(params_bert_tokens_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params[\"callbacks\"] = [get_proper_callback(PlotLossesKeras)]\n",
    "fit_params[\"callbacks\"] += [EarlyStopping(patience=5)]\n",
    "\n",
    "history_rnn = model_bert_rnn.fit(**fit_params)\n",
    "save_report(model_bert_rnn, \"model_bert_rnn\", history_rnn.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bert_tokens_rnn = defaultdict(int)\n",
    "\n",
    "params_bert_tokens_rnn = {\n",
    "    \"is_bert\": True,\n",
    "    \"bert_input_layer\": BertInputLayer,\n",
    "    \"shape\": (1, ),\n",
    "    \"input_dtype\": x_train.dtype,\n",
    "    \"emb_layer\": BertLayer(trainable=True, dict_output=\"sequence_outputs\"),\n",
    "    \"out_units\": np.unique(y_train).shape[0],\n",
    "    \"out_activation\": \"softmax\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"optimizer\": \"rmsprop\"\n",
    "}\n",
    "\n",
    "model_bert_cnn = model_cnn(params_bert_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params[\"callbacks\"] = [get_proper_callback(PlotLossesKeras)]\n",
    "fit_params[\"callbacks\"] += [EarlyStopping(patience=5)]\n",
    "\n",
    "history_cnn = model_bert_cnn.fit(**fit_params)\n",
    "save_report(model_bert_cnn, \"model_bert_cnn\", history_cnn.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Level FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import model_ffnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bert_raw = defaultdict(int)\n",
    "\n",
    "params_bert_tokens_rnn = {\n",
    "    \"is_bert\": True,\n",
    "    \"bert_input_layer\": BertInputLayer,\n",
    "    \"shape\": (1, ),\n",
    "    \"input_dtype\": x_train.dtype,\n",
    "    \"emb_layer\": BertLayer(trainable=True, dict_output=\"pooled_output\"),\n",
    "    \"out_units\": np.unique(y_train).shape[0],\n",
    "    \"out_activation\": \"softmax\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"optimizer\": \"rmsprop\"\n",
    "}\n",
    "\n",
    "model_elmo_ffnn = model_ffnn(params_bert_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params[\"callbacks\"] = [get_proper_callback(PlotLossesKeras)]\n",
    "fit_params[\"callbacks\"] += [EarlyStopping(patience=5)]\n",
    "\n",
    "history_ffnn = model_bert_ffnn.fit(**fit_params)\n",
    "save_report(model_bert_ffnn, \"model_bert_ffnn\", history_ffnn.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
